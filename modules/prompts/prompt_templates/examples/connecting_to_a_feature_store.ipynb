{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a792b119",
   "metadata": {},
   "source": [
    "# Connecting to a Feature Store\n",
    "\n",
    "Feature stores are a concept from traditional machine learning that make sure data fed into models is up-to-date and relevant. For more on this, see [here](https://www.tecton.ai/blog/what-is-a-feature-store/).\n",
    "\n",
    "This concept is extremely relevant when considering putting LLM applications in production. In order to personalize LLM applications, you may want to combine LLMs with up-to-date information about particular users. Feature stores can be a great way to keep that data fresh, and LangChain provides an easy way to combine that data with LLMs.\n",
    "\n",
    "In this notebook we will show how to connect prompt templates to feature stores. The basic idea is to call a feature store from inside a prompt template to retrieve values that are then formatted into the prompt."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad0b5edf",
   "metadata": {},
   "source": [
    "## Feast\n",
    "\n",
    "To start, we will use the popular open source feature store framework [Feast](https://github.com/feast-dev/feast).\n",
    "\n",
    "This assumes you have already run the steps in the README around getting started. We will build of off that example in getting started, and create and LLMChain to write a note to a specific driver regarding their up-to-date statistics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f02f6f3",
   "metadata": {},
   "source": [
    "### Load Feast Store\n",
    "\n",
    "Again, this should be set up according to the instructions in the Feast README"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd1a452a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from feast import FeatureStore\n",
    "\n",
    "# You may need to update the path depending on where you stored it\n",
    "feast_repo_path = \"../../../../../my_feature_repo/feature_repo/\"\n",
    "store = FeatureStore(repo_path=feast_repo_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe8aae5",
   "metadata": {},
   "source": [
    "### Prompts\n",
    "\n",
    "Here we will set up a custom FeastPromptTemplate. This prompt template will take in a driver id, look up their stats, and format those stats into a prompt.\n",
    "\n",
    "Note that the input to this prompt template is just `driver_id`, since that is the only user defined piece (all other variables are looked up inside the prompt template)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e9cee04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate, StringPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "594a3cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Given the driver's up to date stats, write them note relaying those stats to them.\n",
    "If they have a conversation rate above .5, give them a compliment. Otherwise, make a silly joke about chickens at the end to make them feel better\n",
    "\n",
    "Here are the drivers stats:\n",
    "Conversation rate: {conv_rate}\n",
    "Acceptance rate: {acc_rate}\n",
    "Average Daily Trips: {avg_daily_trips}\n",
    "\n",
    "Your response:\"\"\"\n",
    "prompt = PromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8464c731",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeastPromptTemplate(StringPromptTemplate):\n",
    "    \n",
    "    def format(self, **kwargs) -> str:\n",
    "        driver_id = kwargs.pop(\"driver_id\")\n",
    "        feature_vector = store.get_online_features(\n",
    "            features=[\n",
    "                'driver_hourly_stats:conv_rate',\n",
    "                'driver_hourly_stats:acc_rate',\n",
    "                'driver_hourly_stats:avg_daily_trips'\n",
    "            ],\n",
    "            entity_rows=[{\"driver_id\": 1001}]\n",
    "        ).to_dict()\n",
    "        kwargs[\"conv_rate\"] = feature_vector[\"conv_rate\"][0]\n",
    "        kwargs[\"acc_rate\"] = feature_vector[\"acc_rate\"][0]\n",
    "        kwargs[\"avg_daily_trips\"] = feature_vector[\"avg_daily_trips\"][0]\n",
    "        return prompt.format(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c0c7bae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = FeastPromptTemplate(input_variables=[\"driver_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d8d70bb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given the driver's up to date stats, write them note relaying those stats to them.\n",
      "If they have a conversation rate above .5, give them a compliment. Otherwise, make a silly joke about chickens at the end to make them feel better\n",
      "\n",
      "Here are the drivers stats:\n",
      "Conversation rate: 0.4745151400566101\n",
      "Acceptance rate: 0.055561766028404236\n",
      "Average Daily Trips: 936\n",
      "\n",
      "Your response:\n"
     ]
    }
   ],
   "source": [
    "print(prompt_template.format(driver_id=1001))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2870d070",
   "metadata": {},
   "source": [
    "### Use in a chain\n",
    "\n",
    "We can now use this in a chain, successfully creating a chain that achieves personalization backed by a feature store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7106255c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "79543326",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = LLMChain(llm=ChatOpenAI(), prompt=prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "97a741a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hi there! I wanted to update you on your current stats. Your acceptance rate is 0.055561766028404236 and your average daily trips are 936. While your conversation rate is currently 0.4745151400566101, I have no doubt that with a little extra effort, you'll be able to exceed that .5 mark! Keep up the great work! And remember, even chickens can't always cross the road, but they still give it their best shot.\""
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.run(1001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e59aaf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
